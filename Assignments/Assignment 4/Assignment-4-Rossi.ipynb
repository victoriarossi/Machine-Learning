{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a3b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fba1665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier weights: \n",
      " [[ 0.9437]\n",
      " [ 0.2137]\n",
      " [ 0.2664]\n",
      " [-0.3922]\n",
      " [-0.0054]\n",
      " [-0.0176]\n",
      " [-0.1663]\n",
      " [-0.0823]\n",
      " [-0.1664]]\n"
     ]
    }
   ],
   "source": [
    "# a)\n",
    "data = loadmat('face_emotion_data.mat')\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "w = np.linalg.inv(X.T@X)@X.T@y\n",
    "print(\"Classifier weights: \\n\",w.round(4))\n",
    "\n",
    "# b) \n",
    "# Use the following steps:\n",
    "# i. Extract the same nine features from the new face image. Place them in a row vector VT\n",
    "# ii. Compute the product y_hat = VT@w , where w is the weigth vector calculated before.\n",
    "# iii. Compute s = sign(y_hat). If s = 1, classify as happy. Otherwise if s = -1, classify as angry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19cbcaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Using SVD\n",
    "U,s,VT = np.linalg.svd(X, full_matrices= False)\n",
    "S = np.arange(81).reshape(9,9)\n",
    "S_matrix = np.zeros_like(S) \n",
    "np.fill_diagonal(S_matrix, s)\n",
    "\n",
    "w = np.transpose(VT)@np.linalg.inv(S_matrix)@np.transpose(U)@y\n",
    "\n",
    "# b) to classify a new face as happy or angry we can use y_hat => -1 as a angry and 1 as happy\n",
    "y_hat = np.sign(X@w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28f70634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.6455856 ],\n",
       "       [-4.16572338],\n",
       "       [-1.63547687],\n",
       "       [ 6.24709159],\n",
       "       [-3.28039126],\n",
       "       [ 0.39327948],\n",
       "       [-0.37757024],\n",
       "       [-0.32271646],\n",
       "       [-0.567622  ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c)\n",
    "w_hat = np.transpose(U)@y\n",
    "w_hat\n",
    "\n",
    "# c) Solution:\n",
    "# The classifier computes a weighted sum of nine features. If the features are v_1, ... , v_9 then the predicted\n",
    "# label (before taking the sign) is:\n",
    "#   y_hat = w_1@v_1 + ... + w_9@v_9\n",
    "# Since the features are normalized - each column of X has a squared norm of 128 - the size of the weigth w_i \n",
    "# determines the amount a feature v_i contributes to th epredicted label. Therefore, if a weigth is small, then\n",
    "# the relative contribution of that feature to the predicted label will be commensurately small. Thus, the\n",
    "# magnitude of the weights indicates the degree of importance of the corresponding feature.\n",
    "\n",
    "# Then, \n",
    "# To see which feature is most important we have to take the biggest abstract on w_hat. That is w_hat[3] = w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dea29100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.24709159]\n",
      " [ 5.6455856 ]\n",
      " [-4.16572338]]\n",
      "new classifier weights:\n",
      " [[ 0.7055]\n",
      " [ 0.8738]\n",
      " [-0.7881]]\n"
     ]
    }
   ],
   "source": [
    "# d)\n",
    "# To design the classifier I'll pick the three most important features from w_hat. Those will be w4, w1 and w2\n",
    "# Then,\n",
    "w_hat_2 = np.array((w_hat[3],w_hat[0],w_hat[1]))\n",
    "y_hat_2 = np.sign(X@w_hat)\n",
    "print(w_hat_2)\n",
    "\n",
    "# d) Solution:\n",
    "# Choose the three features with the largest associated weight magnitudes. These turn our to be features 1, 3\n",
    "# and 4. We may design the classifier based on these three features by first removing the columns of X \n",
    "# corresponding to features we are no longer using. Call this new matriz X_r, the three feature classifier\n",
    "# is designed by solving the least-squares problem X_r@w_r = y.\n",
    "\n",
    "X_r = X[:,[0,2,3]]\n",
    "w_r = np.linalg.inv(X_r.T@X_r)@X_r.T@y\n",
    "print(\"new classifier weights:\\n\", w_r.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "558ced0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "13\n",
      "Error rate is 2.34 using all the features\n",
      "Error rate is 6.25 using only features 1, 3, 4\n"
     ]
    }
   ],
   "source": [
    "# e) \n",
    "# 1)\n",
    "print(len([j for k in (y_hat-y) for j in k if j != 0]))\n",
    "\n",
    "# 2)\n",
    "print(len([j for k in (y_hat_2-y) for j in k if j != 0]))\n",
    "\n",
    "# e) Solution:\n",
    "# 1)\n",
    "y_predict = np.sign(X@w)\n",
    "err1 = np.mean( y != y_predict)\n",
    "print(\"Error rate is %.2f using all the features\" %(err1*100))\n",
    "\n",
    "# 2)\n",
    "y_r_predict = np.sign(X_r@w_r)\n",
    "err2 = np.mean( y != y_r_predict)\n",
    "print(\"Error rate is %.2f using only features 1, 3, 4\" %(err2*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
